
@online{yann_lecun_mnist_nodate,
	title = {{MNIST} handwritten digit database, Yann {LeCun}, Corinna Cortes and Chris Burges},
	url = {http://yann.lecun.com/exdb/mnist/},
	author = {{Yann LeCun} and {Corinna Cortes} and {Christopher J.C. Burges}},
	urldate = {2022-04-30},
	file = {MNIST handwritten digit database, Yann LeCun, Corinna Cortes and Chris Burges:/Users/asworski/Documents/Zotero/storage/3CFWGVBF/mnist.html:text/html},
}

@online{noauthor_cifar-10_nodate,
	title = {{CIFAR}-10 and {CIFAR}-100 datasets},
	url = {https://www.cs.toronto.edu/~kriz/cifar.html},
	urldate = {2022-04-30},
	file = {CIFAR-10 and CIFAR-100 datasets:/Users/asworski/Documents/Zotero/storage/PMJIC8L5/cifar.html:text/html},
}

@online{noauthor_convolutional_nodate,
	title = {Convolutional Neural Networks ({CNN}) for {CIFAR}-10 Dataset},
	url = {http://parneetk.github.io/blog/cnn-cifar10/},
	abstract = {More examples to implement {CNN} in Keras},
	titleaddon = {Parneet Kaur},
	urldate = {2022-04-30},
	langid = {english},
	file = {Snapshot:/Users/asworski/Documents/Zotero/storage/8X3XUPGY/cnn-cifar10.html:text/html},
}

@online{noauthor_fig_nodate,
	title = {Fig. 2. Some samples of the {MNIST} dataset},
	url = {https://www.researchgate.net/figure/Some-samples-of-the-MNIST-dataset_fig1_342733731},
	abstract = {Download scientific diagram {\textbar} Some samples of the {MNIST} dataset from publication: {RDP}-{GAN}: A R{\textbackslash}'enyi-Differential Privacy based Generative Adversarial Network {\textbar} Generative adversarial network ({GAN}) has attracted increasing attention recently owing to its impressive ability to generate realistic samples with high privacy protection. Without directly interactive with training examples, the generative model can be fully used to estimate... {\textbar} Privacy, Privatization and Protection {\textbar} {ResearchGate}, the professional network for scientists.},
	titleaddon = {{ResearchGate}},
	urldate = {2022-04-30},
	langid = {english},
	file = {Snapshot:/Users/asworski/Documents/Zotero/storage/WE6IDQT5/Some-samples-of-the-MNIST-dataset_fig1_342733731.html:text/html},
}

@online{lazar_building_2021,
	title = {Building a {ResNet} in Keras},
	url = {https://towardsdatascience.com/building-a-resnet-in-keras-e8f1322a49ba},
	abstract = {Using Keras Functional {API} to construct a Residual Neural Network},
	titleaddon = {Towards Data Science},
	author = {Lazar, Dorian},
	urldate = {2022-04-30},
	date = {2021-05-14},
	langid = {english},
	file = {Snapshot:/Users/asworski/Documents/Zotero/storage/3P7RG6V6/building-a-resnet-in-keras-e8f1322a49ba.html:text/html},
}

@online{le_implementation_2021,
	title = {Implementation of {GoogLeNet} on Keras},
	url = {https://medium.com/mlearning-ai/implementation-of-googlenet-on-keras-d9873aeed83c},
	abstract = {1. Introduction},
	titleaddon = {{MLearning}.ai},
	author = {Le, Khuyen},
	urldate = {2022-04-30},
	date = {2021-12-08},
	langid = {english},
	file = {Snapshot:/Users/asworski/Documents/Zotero/storage/D2UF5L5M/implementation-of-googlenet-on-keras-d9873aeed83c.html:text/html},
}

@online{noauthor_inception_2018,
	title = {Inception Network {\textbar} Implementation Of {GoogleNet} In Keras},
	url = {https://www.analyticsvidhya.com/blog/2018/10/understanding-inception-network-from-scratch/},
	abstract = {Inception network used for solving image recognition and detection problems. Learn about Inception networks and implementation of googlenet in Keras.},
	titleaddon = {Analytics Vidhya},
	urldate = {2022-04-30},
	date = {2018-10-18},
	langid = {english},
	file = {Snapshot:/Users/asworski/Documents/Zotero/storage/KEGKP97F/understanding-inception-network-from-scratch.html:text/html},
}

@online{raj_simple_2020,
	title = {A Simple Guide to the Versions of the Inception Network},
	url = {https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202},
	abstract = {The Inception network was an important milestone in the development of {CNN} classifiers. Prior to its inception (pun intended), most popular…},
	titleaddon = {Medium},
	author = {Raj, Bharath},
	urldate = {2022-04-30},
	date = {2020-07-31},
	langid = {english},
	file = {Snapshot:/Users/asworski/Documents/Zotero/storage/VGPJNAEH/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202.html:text/html},
}

@online{mohan_keras_2020,
	title = {Keras Implementation of {ResNet}-50 (Residual Networks) Architecture from Scratch},
	url = {https://machinelearningknowledge.ai/keras-implementation-of-resnet-50-architecture-from-scratch/},
	abstract = {In this article we will see Keras implementation of {ResNet} 50 from scratch with Dog vs Cat dataset. We will also understand its architecture.},
	titleaddon = {{MLK} - Machine Learning Knowledge},
	author = {Mohan, Sachin},
	urldate = {2022-04-30},
	date = {2020-12-26},
	langid = {english},
	file = {Snapshot:/Users/asworski/Documents/Zotero/storage/H9CXBCMN/keras-implementation-of-resnet-50-architecture-from-scratch.html:text/html},
}

@article{szegedy_going_2014,
	title = {Going Deeper with Convolutions},
	url = {http://arxiv.org/abs/1409.4842},
	abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the {ImageNet} Large-Scale Visual Recognition Challenge 2014 ({ILSVRC} 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for {ILSVRC} 2014 is called {GoogLeNet}, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
	journaltitle = {{arXiv}:1409.4842 [cs]},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	urldate = {2022-04-30},
	date = {2014-09-16},
	eprinttype = {arxiv},
	eprint = {1409.4842},
	note = {version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/asworski/Documents/Zotero/storage/AB3IN6XX/Szegedy et al. - 2014 - Going Deeper with Convolutions.pdf:application/pdf;arXiv.org Snapshot:/Users/asworski/Documents/Zotero/storage/FDRV44YP/1409.html:text/html},
}

@article{szegedy_rethinking_2015,
	title = {Rethinking the Inception Architecture for Computer Vision},
	url = {http://arxiv.org/abs/1512.00567},
	abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the {ILSVRC} 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error on the validation set (3.6\% error on the test set) and 17.3\% top-1 error on the validation set.},
	journaltitle = {{arXiv}:1512.00567 [cs]},
	author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
	urldate = {2022-04-30},
	date = {2015-12-11},
	eprinttype = {arxiv},
	eprint = {1512.00567},
	note = {version: 3},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/asworski/Documents/Zotero/storage/5QFF2ZDN/Szegedy et al. - 2015 - Rethinking the Inception Architecture for Computer.pdf:application/pdf;arXiv.org Snapshot:/Users/asworski/Documents/Zotero/storage/8CCVNS5N/1512.html:text/html},
}

@article{simonyan_very_2015,
	title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our {ImageNet} Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing {ConvNet} models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	journaltitle = {{arXiv}:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	urldate = {2022-04-30},
	date = {2015-04-10},
	eprinttype = {arxiv},
	eprint = {1409.1556},
	note = {version: 6},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/asworski/Documents/Zotero/storage/MTVUYJVU/Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf;arXiv.org Snapshot:/Users/asworski/Documents/Zotero/storage/M3AYA3YY/1409.html:text/html},
}

@article{he_deep_2015,
	title = {Deep Residual Learning for Image Recognition},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the {ImageNet} dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than {VGG} nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the {ImageNet} test set. This result won the 1st place on the {ILSVRC} 2015 classification task. We also present analysis on {CIFAR}-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the {COCO} object detection dataset. Deep residual nets are foundations of our submissions to {ILSVRC} \& {COCO} 2015 competitions, where we also won the 1st places on the tasks of {ImageNet} detection, {ImageNet} localization, {COCO} detection, and {COCO} segmentation.},
	journaltitle = {{arXiv}:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2022-04-30},
	date = {2015-12-10},
	eprinttype = {arxiv},
	eprint = {1512.03385},
	note = {version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/asworski/Documents/Zotero/storage/WSUNSN4L/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/Users/asworski/Documents/Zotero/storage/WPMREQMD/1512.html:text/html},
}

@online{noauthor_forks_nodate,
	title = {Forks · {HarisIqbal}88/{PlotNeuralNet}},
	url = {https://github.com/HarisIqbal88/PlotNeuralNet},
	abstract = {Latex code for making neural networks diagrams. Contribute to {HarisIqbal}88/{PlotNeuralNet} development by creating an account on {GitHub}.},
	titleaddon = {{GitHub}},
	urldate = {2022-05-01},
	langid = {english},
	file = {Snapshot:/Users/asworski/Documents/Zotero/storage/VSYMWRIC/vgg16.html:text/html},
}

@online{thakur_step_2020,
	title = {Step by step {VGG}16 implementation in Keras for beginners},
	url = {https://towardsdatascience.com/step-by-step-vgg16-implementation-in-keras-for-beginners-a833c686ae6c},
	abstract = {{VGG}16 is a convolution neural net ({CNN} ) architecture which was used to win {ILSVR}(Imagenet) competition in 2014. It is considered to be…},
	titleaddon = {Medium},
	author = {Thakur, Rohit},
	urldate = {2022-05-02},
	date = {2020-11-24},
	langid = {english},
	file = {Snapshot:/Users/asworski/Documents/Zotero/storage/D5HIKG33/step-by-step-vgg16-implementation-in-keras-for-beginners-a833c686ae6c.html:text/html},
}

@online{gervais_answer_2019,
	title = {Answer to "How to fix "{ResourceExhaustedError}: {OOM} when allocating tensor""},
	url = {https://stackoverflow.com/a/59395251},
	shorttitle = {Answer to "How to fix "{ResourceExhaustedError}},
	titleaddon = {Stack Overflow},
	author = {Gervais, Nicolas},
	urldate = {2022-05-02},
	date = {2019-12-18},
	file = {Snapshot:/Users/asworski/Documents/Zotero/storage/5GE43689/how-to-fix-resourceexhaustederror-oom-when-allocating-tensor.html:text/html},
}