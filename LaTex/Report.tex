\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{dirtree}
\usepackage[style=numeric,backend=biber]{biblatex}
\pagestyle{plain}
\addbibresource{references.bib}
\usepackage{hyperref}
\begin{document}

\title{Deeper Networks for Image Classification}

\author{\IEEEauthorblockN{Alexander Sworski}
\IEEEauthorblockA{\textit{ECS795P â€“ Deep Learning and Computer Vision} \\
\textit{School of Electronic Engineering and Computer Science - Department of Computer Science} \\ {M.Sc. Artificial Intelligence}  \\ 
\textit{Queen Mary University}\\
London, United Kingdom \\
a.sworski@se21.qmul.ac.uk \\
210456914}
}

\maketitle

\begin{abstract}
This paper represents a comparison between different Convolutional Neural Networks. In particular, the models GoogLeNet, VGG-16 and ResNet are compared on different datasets, such as MNIST and Cifar10.
\end{abstract}

\begin{IEEEkeywords}
GoogLeNet, ResNet, VGG-16, MNIST, Image Classification, Deep Learning
\end{IEEEkeywords}

\section{Introduction}\label{C1}
This paper represents a comparison between different Convolutional Neural Networks. In particular, the models GoogLeNet, VGG-16 and ResNet. These three models are compared on the datasets MNIST and Cifar10.
This document is split into 6 chapters. Chapter \ref{C1} represents a short introduction to the topic and serves as an overview of the document.
Chapter \ref{C2} presents a literature review containing a short overview and history of the models and the datasets.
Chapter \ref{C3} describes the implementation of the models, the conducted experiment and the dataset usage.
In Chapter \ref{C4} the findings of the experiments are presented, which are then discussed in Chapter \ref{C5}.
Followed by a conclusion in Chapter \ref{C6}

\section{Literature Review}\label{C2}

\subsection{Neural Network models}
\subsubsection{GoogLeNet}
GoogLeNet was initially developed as a submission for the ImageNet Large-Scale Visual Recognition Challenge 2014, which it won. 
The main hallmark of this model is the improved utilisation of the computing resources inside the network. 
It has 12 times fewer parameters than the winner of 2012 (AlexNet) but performs significantly better. \cite{szegedy_going_2014}
This model first introduced the idea of an inception module, which can be seen in Figure \ref{fig:x inception module 5x5}.
A second version of the model was published a year later, which introduced an improved inception module, as shown in Figure \ref{fig:x inception module 3x3}.
This new version reduced computational costs, as bigger convolutions are disproportionately more expensive. Using two 3x3 convolutions instead of 5x5 is computationally less expensive while also improving the performance.
Although there is also a V3 and V4 of GoogLeNet, for this paper, the V2 Inception has been used.
In total the model has 22 layers.
\begin{figure}[!htbp]
     \centering
     \begin{subfigure}[b]{0.25\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/inceptionv1.png}
         \caption{Original Inception module \cite{szegedy_going_2014}}
         \label{fig:x inception module 5x5}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.22\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/incetionv2.png}
         \caption{V2 Inception module \cite{szegedy_rethinking_2015}}
         \label{fig:x inception module 3x3}
     \end{subfigure}
        \caption{Three simple graphs}
        \label{fig:three graphs}
\end{figure}

\subsubsection{VGG-16}
VGG-16 is the biggest of the models used in this paper (Figure \ref{fig:x mode size}).
It also has been developed as a submission for the ImageNet Challenge, in which it won first and second place for the localisation and classification tracks, respectively.
It consists of 16 layers and uses only 3x3 convolutions. Although it has fewer layers than GoogLeNet, each layer is computationally more complex. 
Ultimately, this model resulted in better scores than the GoogLeNet V1 \cite{simonyan_very_2015}.

\begin{figure}[!htbp]
    \centering
    \includegraphics[scale=0.35]{img/model_sizes.png}
    \caption{top-1 one-crop accuracy versus amount of operations required (circle size represents the amount of parameters) \cite{canziani_analysis_2017}}
    \label{fig:x mode size}
\end{figure}

\subsubsection{ResNet}
ResNet has the highest amount of layers. 
While there are multiple versions, for this paper a 50 layer version has been chosen.
In Figure \ref{fig:x mode size}, it can be observed that the model size is in between GoogLeNet and VGG-16, yet the best result can be expected according to this comparison.
ResNet has been built to ease the training of deep networks. The network design is based on chaining multiple residual learning blocks on a row. One residual learning block can be seen in Figure \ref{fig:x resblock}. \cite{he_deep_2015}

\begin{figure}[!htbp]
    \centering
    \includegraphics[scale=0.2]{img/residual block.png}
    \caption{Residual learning: a building block \cite{he_deep_2015}}
    \label{fig:x resblock}
\end{figure}


\subsection{Image datasets}
\subsubsection{MNIST}
the MNIST database contains 70,000 28x28 black and white images. 
60,000 images are for training and 10,000 images for testing. 
The images portrait handwritten numbers from 0 to 9. \cite{yann_lecun_mnist_nodate} 
Examples of the classes can be seen in Figure \ref{fig:x MNIST image samples}.
\begin{figure}[!htbp]
    \centering
    \includegraphics[scale=0.35]{img/mnist_sample.jpeg}
    \caption{Cifar10 image samples \cite{noauthor_convolutional_nodate}}
    \label{fig:x MNIST image samples}
\end{figure}
\subsubsection{Cifar10}
the CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. 
There are 50000 training images and 10000 test images. 
The classes are mutually exclusive.\cite{noauthor_cifar-10_nodate} 
Examples of the classes can be seen in Figure \ref{fig:x Cifar10 image samples}.
\begin{figure}[!htbp]
    \centering
    \includegraphics[scale=0.45]{img/cifar_10_sample.png}
    \caption{Cifar10 image samples \cite{noauthor_fig_nodate}}
    \label{fig:x Cifar10 image samples}
\end{figure}

\section{Methodology}\label{C3}
In the first subsection \ref{IM} the implementation of the models using the framework Keras will be explained.
In the subsection \ref{ID} it will be explained how the datasets have been made compatible with the models.
\subsection{Model implementation}\label{IM}
\subsubsection{GoogLeNet}
GoogLeNet, considering only the parameters, is the most lightweight model of the three. 
Nevertheless, it has 108 operational layers ordered in 22 layers, which can be logically segmented into five stages, excluding input and output. 
The first two stages of the model consist of multiple 2D convolutions, followed by a \verb|BatchNormalization| layer and a \verb|MaxPooling2D| layer. 
Using the \verb|BatchNormalization| layer is a unique attribute of the second version of GoogLeNet.
Stage 3 consists of two V2 inceptions, as seen in Figure \ref{fig:x inception module 3x3}, followed by a \verb|MaxPooling2D| layer.
Stage 4 has five inceptions and two auxiliary modules, followed by a \verb|MaxPooling2D| layer. 
In Stage 5, we have two inception modules followed by a \verb|GobalAveragePooling2D| layer, equivalent to the 7x7 convolution found in other versions of this network.
The model then finishes with a Dropout of 0.4 and a fully connected layer using \verb|softmax| as its activation function.
\cite{szegedy_rethinking_2015}

\subsubsection{VGG-16}
VGG-16 is the biggest model of the three regarding the parameters. 
Based on layers, this model is the smallest, as it only consists of 16 layers, which are segmented into 5 blocks. 
Each block is structured in the same manner: two or three 2D convolutions, followed by one 2D MaxPolling layer. 
The five blocks are then followed by three dense layers, one dropout of 0.5 and a final fully connected layer using \verb|softmax| as its activation function. 
The full network architecture can be seen in Figure \ref{fig:x VGG architecture}. \cite{simonyan_very_2015}

Unfortunately, this model was too complex for the Google Colab environment, as the error \verb|"ResourceExhaustedError:| \verb|OOM when allocating tensor"| has been given. 
Therefore, the model was modified using suggestions made in the Stack Overflow forum. \cite{gervais_answer_2019}
The alterations were as follows: each block with three convolutions has been reduced to two, each block with two has been reduced to one convolutional layer, and the final three dense layers have been reduced to only one dense layer. Furthermore, the batch size has been reduced to 32. 
The rest of the model was not modified. This led to disappointing results, which can be found in Table \ref{tab: VGG model accuracy} and Figure \ref{fig:x matrix_VGG_CIFAR_SGD}. \footnote{During later stages of the project, while exchanging with classmates, it was brought to my attention, that Google does limit the usage of Colab Pro after a while, which is presumably the cause for this issue. }

Further, testing also confirmed that there was no implementation error in the original Keras implementation. 
Therefore, the only remaining alternative was to implement the model in a different library, in this case, PyTorch, which is more lightweight.
The implementation of the model, with the initial architecture using PyTorch instead of Keras,  could then be successfully executed in Colab. 
The only minor change that has been made, was to reduce the image size of the input for this model to 64x64 pixels, as the environment would otherwise run out of RAM.

\begin{figure}[!htbp]
    \centering
    \includegraphics[scale=0.07]{img/VGG.png}
    \caption{VGG-16 architecture \cite{noauthor_forks_nodate}}
    \label{fig:x VGG architecture}
\end{figure}


\subsubsection{ResNet}
for this paper, the ResNet-50 Model has been chosen. 
The number 50 means that the model consists of 50 layers of residual blocks (Figure \ref{fig:x resblock}) stacked on top of each other. 
As mentioned before, there are different versions of ResNet, which all hold a different amount of layers. 
Based on the original paper results, ResNet-50 has the second-best results, behind ResNet-110. \cite{he_deep_2015}.  
For this paper, the ResNet version designed for the ImageNet dataset has been used. There is a version in the original paper that has been adapted to use (32,32,3). 
In an effort to keep results between the networks comparable, the version accepting smaller images has not been used.
The main final network architecture consists of 4 blocks, each starting with a convolutional block, followed by a varying number of identity blocks (2,3,5,2).

Unfortunately, this ResNet implementation experienced the same issues as the original VGG-16 implementation in Keras. Consequently, the model was reimplemented using PyTorch.

\subsection{The Datasets}\label{ID}

The datasets used in this paper both have a resolution of 28x28 pixels. 
On top, the Cifar10 dataset has three colour channels, while the MNIST dataset only has one grey channel.
This represents an issue, as the neural networks have initially been designed to work with the Microsoft ImageNet dataset. 
The ImageNet dataset has three colour channels and a resolution of 224x224 pixels.
The datasets have been altered as follows to fit the input dimensions of the models.
As previously mentioned, for the PyTorch model, the datasets have only been upscaled to the dimensions (64,64,3).
They have then been wrapped within a DataLoader class.

\subsubsection{MNIST dataset alterations}
the MNIST dataset has the shape (28,28,1), while our models required an input shape of (224,224,3). 
In order to change the dimensionality, the OpenCV library has been used. Using OpenCV, the image has been interpolated to fit the 224x224 image size. 
Afterwards, the image was stacked three times to obtain our three-channel input. Although, there is the OpenCV function \verb|cv2.cvtColor(src, cv2.COLOR_GRAY2RGB)|, 
which can convert from greyscale to RGB, the results are not different from our stacked layers. 
This is due to the unique properties the MNIST dataset has. Using the stacked layers is computational lightweight.

\subsubsection{Cifar10 dataset alterations}
the Cifar10 dataset is already in RGB. Therefore, we do not need to convert it into a different colour space. 
Therefore, this dataset has been interpolated from the shape (28,28,3) to (224,224,3) using the OpenCV library.

\subsection{Project structure}
This project aims to create a GitHub repository that is fully documented and allows for easy replication of the results presented in this paper. 
In Figure \ref{dir: file strucutre}, the file structure of this project can be seen.
All project files are public on \href{https://github.com/devasworski/Deeper-Networks-for-Image-Classification}{GitHub}.

\begin{figure}[!htbp]
\dirtree{%
 .1 /.
 .2 Checkpoints.
 .3 GoogLeNet.
 .3 ResNet.
 .3 VGG.
 .2 GoogleNet.ipynb.
 .2 VGG\_torch.ipynb.
 .2 VGG.ipynb.
 .2 ResNet.ipynb.
 .2 ResNet\_torch.ipynb.
 .2 py.
 .3 ResNet.py.
 .3 ResNet\_torch.py.
 .3 googLeNet.py.
 .3 VGG.py.
 .3 VGG\_torch.py.
 .3 helper.py.
 .3 datasets.py.
 .3 helper\_torch.py.
 .3 datasets\_torch.py.
 .2 Latex.
 .3 Report.tex.
 .3 Report.pdf.
 .3 img.
}
\caption{Project file structure}
\label{dir: file strucutre}
\end{figure}

The project has five entry points, which are each a Jupiter notebook file. 
Each notebook is dedicated to one neural network.
At the top of the notebooks, the hyperparameters of the network can be adjusted as well as runtime environment variables.
The notebooks have two modes, one for a Google Colab execution and one for local execution.
In the case of a Google Colab execution, the notebook will use Google Drive for checkpoints rather than the checkpoint folder within the project.
In order to minimise the code within the notebooks and create a clean and easing readable code, parts of it have been outsourced into dedicated python files.
The \verb|helper.py| \& \verb|helper_torch.py| files take care of hyperparameters as well as evaluate the model. 
The \verb|dataset.py| \& \verb|dataset_torch.py| files handle the dataset download and the alterations described in Section \ref{ID}.
The other python files define the neural networks, corresponding to their names. Every function within the python files is documented, and the notebooks are divided into descriptive sections.


\section{Results}\label{C4}
Each model has been trained over 20 epochs twice for each dataset, once using the Adam optimiser and once using the SGD optimiser. 
The overall accuracy and the confusion matrix have been recorded. 
In the following chapter, the results will be presented.

\subsection{GoogLeNet}

Using the SGD optimiser, the model performed very well on the MNIST dataset but was suboptimal on the CIFAR-10 dataset. 
If we look at the confusion matrix of the model using the SGD optimiser on the CIFAR-10 dataset, we can see that the low accuracy comes from a localised inadequate recognition of the labels bird, cat \& deer, which even as a human are very difficult to distinguish from each other.

We can see a significant reduction in accuracy and a double in training speed with the Adam optimiser.
In Figure \ref{fig:x imatrix_GoogLeNet_MNIST_Adam} \& \ref{fig:x matrix_GoogLeNet_CIFAR_Adam} we can clearly see, that using the Adam optimiser in both cases led to no training at all.

\begin{figure}[!htbp]
    \centering
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/matrx_GoogLeNet_MNIST_Adam.png}
        \caption{Confusion Matrix GoogLeNet MNIST Adam}
        \label{fig:x imatrix_GoogLeNet_MNIST_Adam}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/matrix_GoogLeNet_MNIST_SGD.png}
        \caption{Confusion Matrix GoogLeNet MNIST SGD}
        \label{fig:x matrix_GoogLeNet_MNIST_SGD}
    \end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/matrix_GoogLeNet_CIFAR_adam.png}
        \caption{Confusion Matrix GoogLeNet Cifar10 Adam}
        \label{fig:x matrix_GoogLeNet_CIFAR_Adam}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/matrix_GoogLeNet_CIFAR_SGD.png}
        \caption{Confusion Matrix GoogLeNet Cifar10 SGD}
        \label{fig:x matrix_GoogLeNet_CIFAR_SGD}
    \end{subfigure}
    \caption{GoogLeNet Confusion Matrices}
    \label{fig:GoogLeNet Confusion Matrixis}
\end{figure}

\begin{table}[!htbp]
    \caption{GoogLeNet model Results}
    \begin{center}
    \begin{tabular}{|c|c|c|c|c|}
    \cline{1-5} 
    \multicolumn{3}{|c|}{\textbf{Model accuracy}} & \multicolumn{2}{|c|}{\textbf{time/step}} \\
    \hline 
    \textit{} & \textbf{\textit{Adam}} & \textbf{\textit{SGD}} & \textbf{\textit{Adam}} & \textbf{\textit{SGD}} \\
    \hline
    \textbf{\textit{MNIST}} & 11.02\% & 98.96\% & 133s 354ms & 258s 688ms \\
    \hline
    \textbf{\textit{CIFAR-10}} & 10.17\% & 66.34\% & 110s 351ms & 109s 349ms \\
    \cline{1-5} 
    \end{tabular}
    \label{tab: GoogLeNet model accuracy}
    \end{center}
\end{table}

\subsection{VGG-16}
As previously mentioned, the initial model implemented in Keras was too big to be executed using Google Colab Pro and only a heavily simplified version of the model, which expectedly performed very bad, could be executed.
As this poor performance of the simplified version becomes evident very quickly, the model has only been trained on the CIFAR dataset using the SGD optimiser. The results can be seen in Table \ref{tab: VGG model accuracy} and Figure \ref{fig:x matrix_VGG_CIFAR_SGD}.

\begin{figure}[!htbp]
    \centering
    \includegraphics[scale=0.10]{img/matrix_VGG_CIFAR_SGD.png}
    \caption{Confusion Matrix simplified VGG-16 Cifar10 SGD}
    \label{fig:x matrix_VGG_CIFAR_SGD}
\end{figure}

\begin{table}[!htbp]
    \caption{Simplified VGG-16 model Results}
    \begin{center}
    \begin{tabular}{|c|c|c|}
    \cline{1-3} 
    \multicolumn{2}{|c|}{\textbf{Model accuracy}} & \multicolumn{1}{|c|}{\textbf{time/step}} \\
    \hline 
    \textit{}  & \textbf{\textit{SGD}}  & \textbf{\textit{SGD}} \\
    \hline
    \textbf{\textit{CIFAR-10}} & 9.33\% & 152s 121ms \\
    \cline{1-3} 
    \end{tabular}
    \label{tab: VGG model accuracy}
    \end{center}
\end{table}

To fix this issue, the model has been reimplemented in PyTorch.
As previously mentioned, this model only uses images with the size of 64x64 pixels as input.
Despite that, the model performed well on both the MNIST and the CIFAR dataset using the SGD optimiser. 

\begin{figure}[!htbp]
    \centering
    \begin{subfigure}[b]{0.21\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/matrix_vgg_torch_mnist_adam.png}
        \caption{Confusion Matrix VGG-16 PyTorch MNIST Adam}
        \label{fig:x imatrix_GG-16_PyTorch_MNIST_Adam}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.21\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/matrix_vgg_torch_MNIST_SGD.png}
        \caption{Confusion Matrix VGG-16 PyTorch MNIST SGD}
        \label{fig:x matrix_GG-16_PyTorch_MNIST_SGD}
    \end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/matrix_vgg_torch_CIFAR_Adam.png}
        \caption{Confusion Matrix VGG-16 PyTorch Cifar10 Adam}
        \label{fig:x matrix_VGG-16_PyTorch_CIFAR_Adam}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/matrix_vgg_torch_CIFAR_SGD.png}
        \caption{Confusion Matrix VGG-16 PyTorch Cifar10 SGD}
        \label{fig:x matrix_VGG-16_PyTorch_CIFAR_SGD}
    \end{subfigure}
    \caption{VGG-16 PyTorch Confusion Matrices}
    \label{fig: VGG-16 PyTorch Confusion Matrixis}
\end{figure}

The confusion matrices for the MNIST and the CIFAR dataset can be found in Figure \ref{fig: VGG-16 PyTorch Confusion Matrixis}.
The results can be found in Table \ref{tab: VGG-16 PyTorch model accuracy}.
The results obtained by this model are very similar to the GoogLeNet model, with just slightly better accuracies.

\begin{table}[!htbp]
    \caption{VGG-16 PyTorch model Results}
    \begin{center}
    \begin{tabular}{|c|c|c|c|c|}
    \cline{1-5} 
    \multicolumn{3}{|c|}{\textbf{Model accuracy}} & \multicolumn{2}{|c|}{\textbf{time/step}} \\
    \hline 
    \textit{} & \textbf{\textit{Adam}} & \textbf{\textit{SGD}} & \textbf{\textit{Adam}} & \textbf{\textit{SGD}} \\
    \hline
    \textbf{\textit{MNIST}} & 11.02\% & 99.12\% & 70s 480ms & 70s 200ms  \\
    \hline
    \textbf{\textit{CIFAR-10}} & 9.94\% & 77.10\% & 61s 800ms & 61s 600ms \\
    \cline{1-5} 
    \end{tabular}
    \label{tab: VGG-16 PyTorch model accuracy}
    \end{center}
\end{table}

\subsection{ResNet-50}
While the Keras version of the model was not executable on Google Colab, the PyTorch version did.
In Table \ref{tab: ResNet-50 model accuracy} the accuracy and the training time can be seen, while in Figure \ref{fig: ResNet Confusion Matrixis} the confusion matrices for the model can be found.
The accuracy of the model differs from the accuracy of the others, as this model performs equally well using both the SGD and the Adam optimiser.

\begin{table}[!htbp]
    \caption{ResNet-50 PyTorch model Results}
    \begin{center}
    \begin{tabular}{|c|c|c|c|c|}
    \cline{1-5} 
    \multicolumn{3}{|c|}{\textbf{Model accuracy}} & \multicolumn{2}{|c|}{\textbf{time/step}} \\
    \hline 
    \textit{} & \textbf{\textit{Adam}} & \textbf{\textit{SGD}} & \textbf{\textit{Adam}} & \textbf{\textit{SGD}} \\
    \hline
    \textbf{\textit{MNIST}} & 98.73\% & 99.18\% & 51s & 49s 200ms  \\
    \hline
    \textbf{\textit{CIFAR-10}} & 65.99\% & 71.72\% & 42s 600ms & 66s 656ms \\
    \cline{1-5} 
    \end{tabular}
    \label{tab: ResNet-50 model accuracy}
    \end{center}
\end{table}

\begin{figure}[!htbp]
    \centering
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/matrix_resnet_MNIST_Adam.png}
        \caption{Confusion Matrix ResNet-50 MNIST Adam}
        \label{fig:x imatrix_ResNet_MNIST_Adam}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/matrix_resnet_MNIST_Adam.png}
        \caption{Confusion Matrix ResNet-50 MNIST SGD}
        \label{fig:x matrix_ResNet_MNIST_SGD}
    \end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/matrix_resnet_CIFAR_Adam.png}
        \caption{Confusion Matrix ResNet-50 Cifar10 Adam}
        \label{fig:x matrix_ResNet_CIFAR_Adam}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/matrix_resnet_CIFAR_SGD.png}
        \caption{Confusion Matrix ResNet-50 Cifar10 SGD}
        \label{fig:x matrix_ResNet_CIFAR_SGD}
    \end{subfigure}
    \caption{ResNet-50 PyTorch Confusion Matrices}
    \label{fig: ResNet Confusion Matrixis}
\end{figure}

\section{Discussion}\label{C5}
Two major observations were made during the experiments. 
One is the bad performance of the Adam optimiser, the other is the bad performance of all classifiers on the bird, cat \& deer or dog classes of the CIFAR dataset.

While literature suggests that the Adam optimiser has worse generalisation performance than SGD \cite{gupta_adam_nodate}, the results found in this experiment come as a surprise.
As the models using the SGD optimiser perform very well, it could be argued, that this scenario is a special case, in which the Adam optimiser performs particularly bad.
The argument that Adam is not well suited for Deep Learning and that smaller batch sizes are required to achieve well performance could also be disproven during testing of the GoogLeNet model with Batch sizes as small as one.
The one exception to this behaviour is the ResNet-50 model, which performs similarly well using both the Adam and the SGD optimiser.

The second observation was the issues in distinguishing the bird, cat \& deer or dog classes if the CIFAR dataset. 
While the model all perform excellently in the other classes, these classes seem to be more difficult to classify.
While this could be an indication of a minority class, this can be out ruled as the dataset is pre-revisioned and each class is equally represented.
Neither overfitting of a particular class can be observed, but rather a well-distributed confusion between these classes.
This leads to the conclusion that these classes are too difficult to distinguish for the models. 
The argument is that these images often contain too little information to distinguish them. 
To test this argument, a small test group of 10 people has been asked individually to classify a picture of the bird, car and deer category, given the 10 labels.
While these results are not representative, 30\% of the people were not able to correctly classify the category and nobody could classify the bird correctly, resulting in a performance of 56\%.
Based on these non-representative results, it can be argued that the results with the CIFAR dataset are very well.

Looking back at the initial assumption based on Figure \ref{fig:x mode size}, it can be said that this did almost correspond to the finding.
The ResNet model performed overall the best and the GoogLeNet model performed the worst, with the exception that the VGG-16 model performed 0.06\% better on the MNIST dataset compared to the ResNet model. 
This is within the margin of error.
\section{Conclusion}\label{C6}
While the results obtained in the experiment differed slightly from the initial assumption, the overall experiment can be considered successful.
All three models have been successfully implemented and trained, using both datasets.

In retrospect, the one change that would be made, should this or a similar project be attempted in the future, is to use the PyTorch library instead of the Keras framework.
While the implementation in both Keras and PyTorch was successful, the Keras implementation caused issues when executing on the Google Colab GPU, which cost a significant amount of time.

\printbibliography
\end{document}
